# -*- coding: utf-8 -*-
"""model_train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nx7tOOK6zm3Q22e471kiXlTA9Pqb_fPL
"""

# !pip -q install transformers==4.41.2 datasets==2.20.0 accelerate==0.33.0 safetensors einops

#uploading dataset for training model
from google.colab import files, pathlib
from transformers import AutoTokenizer
import torch
from transformers import AutoModelForCausalLM
from transformers import Trainer, TrainingArguments
import json, pandas as pd
from datasets import Dataset
from transformers import DataCollatorForLanguageModeling


up = files.upload()  # choose your local file data/finetune/train.jsonl
DATA_PATH = next(iter(up.keys()))
print("Loaded:", DATA_PATH)


rows = []
with open(DATA_PATH) as f:
    for line in f:
        ex = json.loads(line)
        inp = ex["input"].strip()
        tgt = ex["target"].strip()  

        # Simple instruction format
        system = "You are a precise extractor. Return ONLY valid JSON with keys: customer_name, issue, priority."
        user   = f"Extract JSON from:\n---\n{inp}\n---"
        text   = f"[SYSTEM]\n{system}\n[/SYSTEM]\n[USER]\n{user}\n[/USER]\n[ASSISTANT]\n{tgt}"
        rows.append({"text": text})

ds = Dataset.from_pandas(pd.DataFrame(rows))
ds = ds.shuffle(seed=7)
print(ds[0]["text"][:400])

BASE_MODEL = "HuggingFaceTB/SmolLM2-135M"
tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

MAX_LEN = 512

def tok_fn(batch):
    out = tokenizer(
        batch["text"],
        truncation=True,
        padding="max_length",   # ensure uniform length
        max_length=MAX_LEN
    )
    out["labels"] = out["input_ids"].copy()
    return out

tok_ds = ds.map(tok_fn, batched=True, remove_columns=["text"])
tok_ds = tok_ds.train_test_split(test_size=0.05, seed=7)  # small eval split
print(tok_ds)

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    device_map="auto",
    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float16,
)


OUT_DIR = "/content/slm_ft"

args = TrainingArguments(
    output_dir=OUT_DIR,
    num_train_epochs=5,                
    per_device_train_batch_size=1,      
    gradient_accumulation_steps=16,     
    learning_rate=2e-4,
    weight_decay=0.0,
    logging_steps=25,
    save_strategy="epoch",
    evaluation_strategy="epoch",
    bf16=torch.cuda.is_available(),
    fp16=not torch.cuda.is_available(), # on CPU fallback
    report_to="none",
)


# dynamic padding collator
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False  
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=tok_ds["train"],
    eval_dataset=tok_ds["test"],
    tokenizer=tokenizer,
    data_collator=data_collator  
)

trainer.train()
trainer.save_model(OUT_DIR)
tokenizer.save_pretrained(OUT_DIR)
print("âœ… saved fine-tuned model to", OUT_DIR)

#code to download tained model from colab
# !cd /content && zip -r slm_ft.zip slm_ft
# from google.colab import files
# files.download("/content/slm_ft.zip")